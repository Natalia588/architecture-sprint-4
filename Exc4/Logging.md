# Архитектурное решение по логированию

## Анализ

Сбор логов потребуется из всех трех приложений - Shop, CRM, MES.

- INFO логами должны быть покрыты все стадии работы с заказом.
  - Создание заказа - Shop API (INFO date/time, orderId, orderStatus, userId)
  - Чтение заказа - Shop API (INFO date/time, orderId, orderStatus, userId)
  - Загрузка файла модели - Shop API (INFO date/time, orderId, filename, size)
  - Вычисление стоимости начато - MES API (INFO date/time, orderId, orderStatus, filename)
  - Вычисление стоимости закончено - MES API (INFO date/time, orderId, orderStatus, filename)
  - Заказ подтвержден - CRM API (INFO date/time, orderId, orderStatus, crmUserId)
  - Запись/чтение сообщений из Rabbit - CRM API, MES API (INFO date/time, orderId, message)
  - Заказ взят в производство - MES API (INFO date/time, orderId, orderStatus, operatorId)
  - Заказ сделан - MES API (INFO date/time, orderId, orderStatus, operatorId)
  - Упаковка/отправка - MES API (INFO date/time, orderId, orderStatus, operatorId)
  - Закрытие - CRM API (INFO date/time, orderId, orderStatus, crmUserId)


- WARN логи будем использовать для состояний, близких к аномальным, например:
  - Размер файла слишком большой для загрузки - нужно уменьшить
  - Формат файла не соответствует допустимому


- ERROR логи - для ошибок, исключений, внештатных ситуаций, например:
  - Файл не загружен - ошибка соединения/базы данных
  - Заказ не сохранен в базу
  - Произошло исключение при расчете стоимости


- DEBUG логи - для отладочных сценариев, в продакшне они по умолчанию выключены, могут потребоваться 
для анализа входных данных


- TRACE логи - для логгирования детальной информации при разработке, тестировании, на продакшне их не включают


## Мотивация
Логи необходимы для получения детальной информации о проблемах, особенно в ситуациях, когда их сложно воспроизвести локально.
Без грамотного логирования невозможно анализировать жалобы клиентов на неправильную работу системы.
Логи помогают восстановить картину произошедшего инцидента.

## Предлагаемое решение
Для сбора, хранения и анализа логов планируется использовать ELK: 
Filebeat, Logstash, Elasticsearch, Kibana.

```markdown
[diagram С4](./jewerly_c4_model.jpg)
```

Filebeat получает данные из приложений, передает их в конвейер Logstach,
затем данные попадают в Elasticsearch на хранение. Kibana используется для визуализации
полученных данных.

Чувствительные данные в логах будут маскироваться, маскировка будет происходить на этапе создания логов (в приложении).

Отдельный индекс планируется выделить для каждого сервиса. 
Все логи будут собираться в едином кластере для удобного анализа.


## Анализ логов
Анализ удобно будет проводить инструментами Kibana - можно создавать графики, настроить алерты
по определенным показателям

